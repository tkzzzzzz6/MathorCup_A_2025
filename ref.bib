
%问题一
@article{1,
    journal = {Nature Machine Intelligence},
    shortjournal = {Nat. Mach. Intell.},
    issn = {2522-5839},
    number = {3},
    author = {Lu Lu and Pengzhan Jin and G. Pang and Zhongqiang Zhang and G. Karniadakis},
    title = {Learning nonlinear operators via DeepONet based on the universal approximation theorem of operators},
    publisher = {Springer Science and Business Media LLC},
    language = {en},
    year = {2019},
    month = {10},
    volume = {3},
    pages = {218--229},
    doi = {10.1038/s42256-021-00302-5},
    url = {https://www.nature.com/articles/s42256-021-00302-5},
    abstract = {It is widely known that neural networks (NNs) are universal approximators of continuous functions. However, a less known but powerful result is that a NN with a single hidden layer can accurately approximate any nonlinear continuous operator. This universal approximation theorem of operators is suggestive of the structure and potential of deep neural networks (DNNs) in learning continuous operators or complex systems from streams of scattered data. Here, we thus extend this theorem to DNNs. We design a new network with small generalization error, the deep operator network (DeepONet), which consists of a DNN for encoding the discrete input function space (branch net) and another DNN for encoding the domain of the output functions (trunk net). We demonstrate that DeepONet can learn various explicit operators, such as integrals and fractional Laplacians, as well as implicit operators that represent deterministic and stochastic differential equations. We study different formulations of the input function space and its effect on the generalization error for 16 different diverse applications. Neural networks are known as universal approximators of continuous functions, but they can also approximate any mathematical operator (mapping a function to another function), which is an important capability for complex systems such as robotics control. A new deep neural network called DeepONet can lean various mathematical operators with small generalization error.},
    urldate = {2025-04-21}
}

@article{2,
    journal = {arXiv.org},
    author = {Angel X. Chang and Thomas Funkhouser and Leonidas Guibas and Pat Hanrahan and Qixing Huang and Zimo Li and Silvio Savarese and Manolis Savva and Shuran Song and Hao Su and Jianxiong Xiao and Li Yi and Fisher Yu},
    title = {ShapeNet: An Information-Rich 3D Model Repository},
    year = {2015},
    month = {12},
    doi = {10.48550/arXiv.1512.03012},
    abstract = {We present ShapeNet: a richly-annotated, large-scale repository of shapesrepresented by 3D CAD models of objects. ShapeNet contains 3D models from amultitude of semantic categories and organizes them under the WordNet taxonomy.It is a collection of datasets providing many semantic annotations for each 3Dmodel such as consistent rigid alignments, parts and bilateral symmetry planes,physical sizes, keywords, as well as other planned annotations. Annotations aremade available through a public web-based interface to enable datavisualization of object attributes, promote data-driven geometric analysis, andprovide a large-scale quantitative benchmark for research in computer graphicsand vision. At the time of this technical report, ShapeNet has indexed morethan 3,000,000 models, 220,000 models out of which are classified into 3,135categories (WordNet synsets). In this report we describe the ShapeNet effort asa whole, provide details for all currently available datasets, and summarizefuture plans.},
    urldate = {2025-04-21}
}

@article{3,
    journal = {arXiv.org},
    author = {Binyang Song and Chenyang Yuan and Frank Permenter and Nikos Arechiga and Faez Ahmed},
    title = {Surrogate Modeling of Car Drag Coefficient with Depth and Normal  Renderings},
    year = {2023},
    month = {05},
    doi = {10.48550/arXiv.2306.06110},
    abstract = {Generative AI models have made significant progress in automating thecreation of 3D shapes, which has the potential to transform car design. Inengineering design and optimization, evaluating engineering metrics is crucial.To make generative models performance-aware and enable them to createhigh-performing designs, surrogate modeling of these metrics is necessary.However, the currently used representations of three-dimensional (3D) shapeseither require extensive computational resources to learn or suffer fromsignificant information loss, which impairs their effectiveness in surrogatemodeling. To address this issue, we propose a new two-dimensional (2D)representation of 3D shapes. We develop a surrogate drag model based on thisrepresentation to verify its effectiveness in predicting 3D car drag. Weconstruct a diverse dataset of 9,070 high-quality 3D car meshes labeled by dragcoefficients computed from computational fluid dynamics (CFD) simulations totrain our model. Our experiments demonstrate that our model can accurately andefficiently evaluate drag coefficients with an $R^2$ value above 0.84 forvarious car categories. Moreover, the proposed representation method can begeneralized to many other product categories beyond cars. Our model isimplemented using deep neural networks, making it compatible with recent AIimage generation tools (such as Stable Diffusion) and a significant steptowards the automatic generation of drag-optimized car designs. We have madethe dataset and code publicly available athttps://decode.mit.edu/projects/dragprediction/.},
    urldate = {2025-04-21}
}

@article{4,
    journal = {arXiv.org},
    author = {Binyang Song and Chenyang Yuan and Frank Permenter and Nikos Arechiga and Faez Ahmed},
    title = {Surrogate Modeling of Car Drag Coefficient with Depth and Normal  Renderings},
    year = {2023},
    month = {05},
    doi = {10.48550/arXiv.2306.06110},
    abstract = {Generative AI models have made significant progress in automating thecreation of 3D shapes, which has the potential to transform car design. Inengineering design and optimization, evaluating engineering metrics is crucial.To make generative models performance-aware and enable them to createhigh-performing designs, surrogate modeling of these metrics is necessary.However, the currently used representations of three-dimensional (3D) shapeseither require extensive computational resources to learn or suffer fromsignificant information loss, which impairs their effectiveness in surrogatemodeling. To address this issue, we propose a new two-dimensional (2D)representation of 3D shapes. We develop a surrogate drag model based on thisrepresentation to verify its effectiveness in predicting 3D car drag. Weconstruct a diverse dataset of 9,070 high-quality 3D car meshes labeled by dragcoefficients computed from computational fluid dynamics (CFD) simulations totrain our model. Our experiments demonstrate that our model can accurately andefficiently evaluate drag coefficients with an $R^2$ value above 0.84 forvarious car categories. Moreover, the proposed representation method can begeneralized to many other product categories beyond cars. Our model isimplemented using deep neural networks, making it compatible with recent AIimage generation tools (such as Stable Diffusion) and a significant steptowards the automatic generation of drag-optimized car designs. We have madethe dataset and code publicly available athttps://decode.mit.edu/projects/dragprediction/.},
    urldate = {2025-04-21}
}
%问题二
%问题三
%问题四




%5.问题五
@article{5,
    journal = {Nature Machine Intelligence},
    shortjournal = {Nat. Mach. Intell.},
    issn = {2522-5839},
    number = {3},
    author = {Lu Lu and Pengzhan Jin and G. Pang and Zhongqiang Zhang and G. Karniadakis},
    title = {Learning nonlinear operators via DeepONet based on the universal approximation theorem of operators},
    publisher = {Springer Science and Business Media LLC},
    language = {en},
    year = {2019},
    month = {10},
    volume = {3},
    pages = {218--229},
    doi = {10.1038/s42256-021-00302-5},
    url = {https://www.nature.com/articles/s42256-021-00302-5},
    abstract = {It is widely known that neural networks (NNs) are universal approximators of continuous functions. However, a less known but powerful result is that a NN with a single hidden layer can accurately approximate any nonlinear continuous operator. This universal approximation theorem of operators is suggestive of the structure and potential of deep neural networks (DNNs) in learning continuous operators or complex systems from streams of scattered data. Here, we thus extend this theorem to DNNs. We design a new network with small generalization error, the deep operator network (DeepONet), which consists of a DNN for encoding the discrete input function space (branch net) and another DNN for encoding the domain of the output functions (trunk net). We demonstrate that DeepONet can learn various explicit operators, such as integrals and fractional Laplacians, as well as implicit operators that represent deterministic and stochastic differential equations. We study different formulations of the input function space and its effect on the generalization error for 16 different diverse applications. Neural networks are known as universal approximators of continuous functions, but they can also approximate any mathematical operator (mapping a function to another function), which is an important capability for complex systems such as robotics control. A new deep neural network called DeepONet can lean various mathematical operators with small generalization error.},
    urldate = {2025-04-20}
}

@article{6,
    journal = {arXiv.org},
    author = {Edoardo Calvello and Nikola B. Kovachki and Matthew E. Levine and Andrew M. Stuart},
    title = {Continuum Attention for Neural Operators},
    year = {2024},
    month = {06},
    doi = {10.48550/arXiv.2406.06486},
    abstract = {Transformers, and the attention mechanism in particular, have becomeubiquitous in machine learning. Their success in modeling nonlocal, long-rangecorrelations has led to their widespread adoption in natural languageprocessing, computer vision, and time-series problems. Neural operators, whichmap spaces of functions into spaces of functions, are necessarily bothnonlinear and nonlocal if they are universal; it is thus natural to ask whetherthe attention mechanism can be used in the design of neural operators.Motivated by this, we study transformers in the function space setting. Weformulate attention as a map between infinite dimensional function spaces andprove that the attention mechanism as implemented in practice is a Monte Carloor finite difference approximation of this operator. The function spaceformulation allows for the design of transformer neural operators, a class ofarchitectures designed to learn mappings between function spaces, for which weprove a universal approximation result. The prohibitive cost of applying theattention operator to functions defined on multi-dimensional domains leads tothe need for more efficient attention-based architectures. For this reason wealso introduce a function space generalization of the patching strategy fromcomputer vision, and introduce a class of associated neural operators.Numerical results, on an array of operator learning problems, demonstrate thepromise of our approaches to function space formulations of attention and theiruse in neural operators.},
    urldate = {2025-04-22}
}
